{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and GRU with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions and derivatives\n",
    "def sigm(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigm(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    return (1 - np.square(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def drelu(x):\n",
    "    return np.array(x > 0, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        r'''\n",
    "        Simple version of Cho & al. GRU. Here we only on train on batch of size 1.\n",
    "        \n",
    "        Args:\n",
    "            input_size: input size of tensors of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n",
    "            hidden_size: the number of hidden nodes and also the output size\n",
    "            \n",
    "        Examples::\n",
    "        \n",
    "            >>> input = np.ones((10,10))\n",
    "            >>> net = GRU(input_size=10, hidden_size=5)\n",
    "            >>> net.forward(input)\n",
    "        '''\n",
    "        # Weights dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Standard deviation for weights initialisation\n",
    "        wstd = 0.2\n",
    "        \n",
    "        # Reset gate\n",
    "        self.Wr = np.random.randn(hidden_size + input_size, hidden_size)*wstd\n",
    "        self.br = np.zeros((hidden_size,))\n",
    "        \n",
    "        # Update gate\n",
    "        self.Wz = np.random.randn(hidden_size + input_size, hidden_size) * wstd\n",
    "        self.bz = np.zeros((hidden_size,))\n",
    "        \n",
    "        # Hidden proposal (\\hat{h})\n",
    "        self.Whhat = np.random.randn(hidden_size + input_size, hidden_size) * wstd\n",
    "        self.bhhat = np.zeros((hidden_size,))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"input_size: {}\\nhidden_size: {}\".format(self.input_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        r'''\n",
    "        Do the forward pass for a single input (batch size = 1).\n",
    "        \n",
    "        Args:\n",
    "            input (Numpy Array): the expected dimension of input array/tensor is (seq_len, input_size), \n",
    "                where seq_len is the length of the sequence (eg. number of words, number of characters, \n",
    "                length of the time serie) and input_size is the number of features in the input.\n",
    "        \n",
    "        Examples::\n",
    "        \n",
    "            >>> input = np.ones((10,10))\n",
    "            >>> net = GRU(input_size=10, hidden_size=5)\n",
    "            >>> net.forward(input)\n",
    "        '''\n",
    "        seq_len = np.shape(input)[0]\n",
    "        \n",
    "        # Gates output tensors\n",
    "        z = np.zeros((seq_len, self.hidden_size))\n",
    "        r = np.zeros((seq_len, self.hidden_size))\n",
    "        hhat = np.zeros((seq_len, self.hidden_size))\n",
    "        h = np.zeros((seq_len, self.hidden_size))\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            x = np.concatenate((h[i-1,:], input[i,:])) # h[i-1,:] = h_{t-1} and input[i,:] = x_t\n",
    "            \n",
    "            # Reset gate\n",
    "            r[i,:] = sigm(np.dot(x, self.Wr) + self.br)           \n",
    "            # Output gate\n",
    "            z[i,:] = sigm(np.dot(x, self.Wz) + self.bz)\n",
    "            # Hidden state proposal\n",
    "            hhat[i,:] = tanh(np.dot(np.concatenate((r[i,:] * h[i-1,:], input[i,:])), self.Whhat) + self.bhhat)\n",
    "            # Hidden state\n",
    "            h[i,:] = (1 - z[i,:]) * h[i-1,:] + z[i,:] * hhat[i,:]\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def backward(self, input, grad):\n",
    "        dWhhat, dWr, dWz = np.zeros_like(self.Whhat), np.zeros_like(self.Wr), np.zeros_like(self.Wz)\n",
    "        dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "        dhnext = np.zeros_like(h[0])\n",
    "        # Backward prop\n",
    "        for t in reversed(range(len(input))):\n",
    "            # âˆ‚loss/âˆ‚y\n",
    "            #dy = np.copy(p[t])\n",
    "            #dy[targets[t]] -= 1\n",
    "\n",
    "            # âˆ‚loss/âˆ‚Wy and âˆ‚loss/âˆ‚by\n",
    "            #dWy += np.dot(dy, h[t].T)\n",
    "            #dby += dy\n",
    "\n",
    "            # Intermediary derivatives\n",
    "            #dh = np.dot(Wy.T, dy) + dhnext\n",
    "            #dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "            #dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True)\n",
    "\n",
    "            # âˆ‚loss/âˆ‚Wh, âˆ‚loss/âˆ‚Uh and âˆ‚loss/âˆ‚bh\n",
    "            x = np.concat(x[t].T, np.multiply(r[t], h[t-1])\n",
    "            dWh += np.dot(dh_hat_l, x)\n",
    "            dbh += dh_hat_l\n",
    "\n",
    "            # Intermediary derivatives\n",
    "            drhp = np.dot(Uh.T, dh_hat_l)\n",
    "            dr = np.multiply(drhp, h[t-1])\n",
    "            dr_l = dr * sigmoid(r[t], deriv=True)\n",
    "\n",
    "            # âˆ‚loss/âˆ‚Wr, âˆ‚loss/âˆ‚Ur and âˆ‚loss/âˆ‚br\n",
    "            dWr += np.dot(dr_l, x[t].T)\n",
    "            dUr += np.dot(dr_l, h[t-1].T)\n",
    "            dbr += dr_l\n",
    "\n",
    "            # Intermediary derivatives\n",
    "            dz = np.multiply(dh, h[t-1] - h_hat[t])\n",
    "            dz_l = dz * sigmoid(z[t], deriv=True)\n",
    "\n",
    "            # âˆ‚loss/âˆ‚Wz, âˆ‚loss/âˆ‚Uz and âˆ‚loss/âˆ‚bz\n",
    "            dWz += np.dot(dz_l, x[t].T)\n",
    "            dUz += np.dot(dz_l, h[t-1].T)\n",
    "            dbz += dz_l\n",
    "\n",
    "            # All influences of previous layer to loss\n",
    "            dh_fz_inner = np.dot(Uz.T, dz_l)\n",
    "            dh_fz = np.multiply(dh, z[t])\n",
    "            dh_fhh = np.multiply(drhp, r[t])\n",
    "            dh_fr = np.dot(Ur.T, dr_l)\n",
    "\n",
    "            # âˆ‚loss/âˆ‚hð‘¡â‚‹â‚\n",
    "            dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "        return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02295972,  0.23587031, -0.390115  ,  0.1746599 ,  0.17550338],\n",
       "       [ 0.02192101,  0.41689426, -0.52664695,  0.2786006 ,  0.3004084 ],\n",
       "       [ 0.01625149,  0.53003645, -0.57954693,  0.33884566,  0.38846527],\n",
       "       [ 0.01021492,  0.59377058, -0.60233494,  0.37298913,  0.45010513],\n",
       "       [ 0.00447726,  0.62778577, -0.61282582,  0.39184535,  0.49304813],\n",
       "       [-0.00076642,  0.64548838, -0.61772325,  0.40189856,  0.52286429],\n",
       "       [-0.00534615,  0.65464511, -0.6199439 ,  0.40698109,  0.54350852],\n",
       "       [-0.00915993,  0.65942042, -0.62088178,  0.40932951,  0.55776629],\n",
       "       [-0.0122026 ,  0.66196111, -0.62122364,  0.4102295 ,  0.56759042],\n",
       "       [-0.01454462,  0.66335375, -0.62130455,  0.41040527,  0.57434512]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.ones((10,10))\n",
    "\n",
    "net = GRU(input_size=10, hidden_size=5)\n",
    "\n",
    "net.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_torch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        r'''\n",
    "        Simple version of Cho & al. GRU. Here we only on train on batch of size 1.\n",
    "        \n",
    "        Args:\n",
    "            input_size: input size of tensors of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n",
    "            hidden_size: the number of hidden nodes and also the output size\n",
    "            \n",
    "        Examples::\n",
    "        \n",
    "            >>> input = np.ones((10,10))\n",
    "            >>> net = GRU(input_size=10, hidden_size=5)\n",
    "            >>> net.forward(input)\n",
    "        '''\n",
    "        super(GRU_torch, self).__init__()\n",
    "        # Weights dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Standard deviation for weights initialisation\n",
    "        wstd = 0.2\n",
    "        \n",
    "        # Reset gate\n",
    "        self.Wr = torch.nn.Parameter(torch.randn((hidden_size, input_size), requires_grad=True) * wstd)\n",
    "        self.Ur = torch.nn.Parameter(torch.randn((hidden_size, hidden_size), requires_grad=True) * wstd)\n",
    "        self.br = torch.nn.Parameter(torch.zeros((hidden_size,), requires_grad=True))\n",
    "        \n",
    "        # Update gate\n",
    "        self.Wz = torch.nn.Parameter(torch.randn((hidden_size, input_size), requires_grad=True) * wstd)\n",
    "        self.Uz = torch.nn.Parameter(torch.randn((hidden_size, hidden_size), requires_grad=True) * wstd)\n",
    "        self.bz = torch.nn.Parameter(torch.zeros((hidden_size,), requires_grad=True))\n",
    "        \n",
    "        # Hidden proposal (\\hat{h})\n",
    "        self.Whhat = torch.nn.Parameter(torch.randn((hidden_size, input_size), requires_grad=True) * wstd)\n",
    "        self.Uhhat = torch.nn.Parameter(torch.randn((hidden_size, hidden_size), requires_grad=True) * wstd)\n",
    "        self.bhhat = torch.nn.Parameter(torch.zeros((hidden_size,), requires_grad=True))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"input_size: {}\\nhidden_size: {}\".format(self.input_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        r'''\n",
    "        Do the forward pass for a single input (batch size = 1).\n",
    "        \n",
    "        Args:\n",
    "            input (Numpy Array): the expected dimension of input array/tensor is (seq_len, input_size), \n",
    "                where seq_len is the length of the sequence (eg. number of words, number of characters, \n",
    "                length of the time serie) and input_size is the number of features in the input.\n",
    "        \n",
    "        Examples::\n",
    "        \n",
    "            >>> input = np.ones((10,10))\n",
    "            >>> net = GRU(input_size=10, hidden_size=5)\n",
    "            >>> net.forward(input)\n",
    "        '''\n",
    "        \n",
    "        # Hidden state is initially set at 0\n",
    "        h = torch.zeros(self.hidden_size)\n",
    "        \n",
    "        for i in range(np.shape(input)[0]):\n",
    "            # Reset gate\n",
    "            r = torch.sigmoid(self.Wr.matmul(input[i, :]) + self.Ur.matmul(h) + self.br)           \n",
    "            # Output gate\n",
    "            z = torch.sigmoid(self.Wz.matmul(input[i, :]) + self.Uz.matmul(h) + self.bz)\n",
    "            # Hidden state proposal\n",
    "            hhat = torch.tanh(self.Whhat.matmul(input[i, :]) + self.Uhhat.matmul(r * h) + self.bhhat)\n",
    "            # Hidden state\n",
    "            h = (1 - z) * h + z * hhat\n",
    "            \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones((2, 5))\n",
    "net = GRU_torch(5, 10)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "hist_loss = []\n",
    "\n",
    "for i in range(100):\n",
    "    net.zero_grad()\n",
    "    output = net.forward(input)\n",
    "    loss = criterion(output, torch.ones_like(out))\n",
    "    \n",
    "    hist_loss.append(loss.detach().numpy())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12358bfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
